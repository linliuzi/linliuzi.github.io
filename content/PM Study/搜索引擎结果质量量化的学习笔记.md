Title: 搜索引擎结果质量量化的学习笔记
Date: 2014-03-25
Category: PM Study
Tags: 技术, 学习笔记, 转载
Slug: new_20140325

	最近都是学习笔记的节奏哈~~归根到底是我太弱了，有好多要学习的东西~~

	今天转发的是搜索引擎结果质量的量化方法的介绍。
	对原文进行了更符合PM需求的改动，去掉了一些技术实现的细节。
	
	搜索引擎的结果质量如果是人工来判断，主观性较强，需要有一个量化的并且尽量自动化的指标。
	本文主要介绍的就是搜索引擎结果质量的常见评价指标和量化方法。与君共学。

陈运文： <http://www.infoq.com/cn/articles/cyw-evaluate-seachengine-result-quality>

##前言
搜索质量评估是搜索技术研究的基础性工作，也是核心工作之一。评价（Metrics）在搜索技术研发中扮演着重要角色，以至于任何一种新方法与他们的评价方式是融为一体的。  
搜索引擎结果的好坏与否，体现在业界所称的在**相关性（Relevance）**上。相关性的定义包括狭义和广义两方面，狭义的解释是：检索结果和用户查询的相关程度。而从广义的层面，相关性可以理解为用户查询的综合满意度。直观的来看，**从用户进入搜索框的那一刻起，到需求获得满足为止，这之间经历的过程越顺畅，越便捷，搜索相关性就越好。**  本文总结业界常用的相关性评价指标和量化评价方法。  

##Cranfield评价体系
二十世纪五十年代英国Cranfield University提出了一套由**查询样例集、正确答案集、评测指标构成的完整评测方案**，并从此确立了“评价”在信息检索研究中的核心地位。  
Cranfield评价体系由三个环节组成：

	1. 构造查询词集合：抽取代表性的查询词，组成一个规模适当的集合
	2. 构造理想结果：针对查询样例集合，从检索系统的语料库中寻找对应的结果，进行标注（通常人工进行，现在有专门做标注的外包公司）
	3. 评价搜索结果与理想结果的接近程度：将查询词和带有标注信息的语料库输入检索系统，对系统反馈的检索结果，使用预定义好的评价计算公式，用数值化的方法来评价检索系统结果和标注的理想结果的接近程度

##构造查询词集合
Cranfield在各大搜索引擎公司内有广泛的应用。具体应用时，首先需要解决的问题是构造一个测试用的查询词集合。为了使评估符合线上实际情况，查询词的各因子的比例会尽量按照线上比例选取，通常从线上用户的QueryLog文件中自动抽取。

	1. Query的类型：寻址类查询(Navigational):信息类查询(Informational):事务类查询(Transactional)各自的比例分别为12.3%:62.0%:25.7%。(Andrei Broder)
	2. Query的频次：对热门query（高频查询）、长尾query（中低频）也分别占特定的比例。
	3. Query的长短：短query（单term的查询）和长Query（多Term的查询）排序算法往往会有一些不同。

构成查询集合后，使用这些查询词，在不同系统（例如对比百度和Google）或不同技术间（新旧两套Ranking算法的环境）进行搜索，并对结果进行评分，以决定优劣。下面具体谈谈评分的方法。

##相关性评价指标

###Precision-recall（准确率-召回率方法）
信息检索领域最广为人知的评价指标为Precision-Recall（准确率-召回率）方法。该方法从提出至今已经历半个世纪，至今在很多搜索引擎公司的效果评估中使用。  

这个方法由准确率和召回率这两个相互关联的统计量构成：**召回率（Recall）衡量一个查询搜索到所有相关文档的能力，而准确率（Precision）衡量搜索系统排除不相关文档的能力。**  
通俗的解释是：准确率就是算一算你查询得到的结果中有多少是靠谱的；而召回率表示所有靠谱的结果中，有多少被你给找回来了。  
具体的计算是：召回率等于被检索到的文档集合与相关文档集合的交集占相关文档集合的比例，准确率等于被检索到的文档集合与相关文档集合的交集占被检索到的文档集合的比例。  

**召回率和准确率分别反映了检索系统的两个最重要的侧面，而这两个侧面又相互制约。**因为大规模数据集合中，如果期望检索到更多相关的文档，必然需要“放宽”检索标准，因此会导致一些不相关结果混进来，从而使准确率受到影响。类似的，期望提高准确率，将不相关文档尽量去除时，务必要执行更“严格”的检索策略，这样也会使一些相关的文档被排除在外，使召回率下降。  
所以为了更清晰的描述两者间的关系，通常我们将Precison-Recall用曲线的方式绘制出来，可以简称为**P-R diagram**。常见的形式如下图所示。（通常曲线是一个逐步向下的走势，即随着Recall的提高，Precision逐步降低）  
![图片](https://lh4.googleusercontent.com/-R6xvvDGdRLw/UzelurARfNI/AAAAAAAAAUs/63xtZ0MKShs/w426-h321-no/%25E9%2587%258F%25E5%258C%2596%25E6%2590%259C%25E7%25B4%25A2%25E8%25B4%25A8%25E9%2587%258F1.jpg "P-R曲线")

**P-R的变种**  
一些特定搜索应用，会更关注搜索结果中错误的结果。例如，搜索引擎的反作弊系统（Anti-Spam System）会更关注检索结果中混入了多少条作弊结果。学术界把这些错误结果称作假阳性（False Positive）结果，对这些应用，通常选择用**虚报率（Fallout）**来统计，虚报率等于被检索到的文档集合与不相关文档集合的交集占不相关文档集合的比例。  
虚报率和准确率本质是完全相同的，只是分别从正反两方面计算，实际是P-R的一个变种。

**P-R的统一指标**  
为了用一个单一的数字来更直观方便地比较两个搜索系统的整体效果，IR学术界提出了F值度量（F -Measure）的方法，通过Presion和Recall的调和平均数来计算  
![图片](https://lh3.googleusercontent.com/-FOvKaVcmiMw/UzemkhI3ztI/AAAAAAAAAVk/G4qW2lYhU6w/w311-h55-no/%25E9%2587%258F%25E5%258C%2596%25E6%2590%259C%25E7%25B4%25A2%25E8%25B4%25A8%25E9%2587%258F1.1.jpg "P-R统一指标")  
其中参数λ∈(0,1)，用于调节系统对Precision和Recall的平衡程度，通常取λ=0.5。  
这里使用调和平均数而不是通常的几何平均或算术平均，原因是调和平均数强调较小数值的重要性，能敏感的反映小数字的变化，因此更适合用来反映检索效果。  

###P@N方法
点击因素的影响传统的Precision-Recall并不完全适用对搜索引擎的评估，原因是**搜索引擎用户的点击方式有其特殊性**，包括：  
	
	1. 60-65%的查询点击了名列搜索结果前10条的网页； 
	2. 20-25%的人会考虑点击名列11到20的网页； 
	3. 仅有3-4%的会点击名列搜索结果中列第21到第30名的网页 

也就是说，绝大部分用户是不愿意翻页去看搜索引擎给出的后面的结果。  

而即使在搜索结果的首页（通常列出的是前10条结果），用户的点击行为也很有意思，我们通过下面的Google点击热图（Heat Map）来观察（这个热图在二维搜索结果页上通过光谱来形象的表达不同位置用户的点击热度。颜色约靠近红色表示点击强度越高）：  
![图片](https://lh3.googleusercontent.com/-fstoimjSMJc/Uzeluk-hvmI/AAAAAAAAAUw/LayNP2K_MDQ/w459-h504-no/%25E9%2587%258F%25E5%258C%2596%25E6%2590%259C%25E7%25B4%25A2%25E8%25B4%25A8%25E9%2587%258F2.jpg "Google点击热图")  
从图中可以看出，搜索结果的前3条吸引了大量的点击，属于热度最高的部分。也就是说，对搜苏引擎来说，最前的几条结果是最关键的，决定了用户的满意程度。  

![图片](https://lh3.googleusercontent.com/-7UsvcYAmP6g/Uzeluhu2KzI/AAAAAAAAAUo/3qHhKredPqc/w444-h408-no/%25E9%2587%258F%25E5%258C%2596%25E6%2590%259C%25E7%25B4%25A2%25E8%25B4%25A8%25E9%2587%258F3.jpg "Google用户行为分析图")  
康乃尔大学的研究人员通过eye tracking实验获得了更为精确的Google搜索结果的用户行为分析图。从这张图中可以看出，第一条结果获得了56.38%的搜索流量，第二条和第三条结果的排名依次降低，但远低于排名第一的结果。前三条结果的点击比例大约为11:3:2。而**前三条结果的总点击几乎分流了搜索流量的80%**。  
另外的一些有趣的结论是，**点击量并不是按照顺序依次递减的**。排名第七位获得的点击是最少的，原因可能在于用户在浏览过程中下拉页面到底部，这时候就只显示最后三位排名网站，第七名便容易被忽略。而首屏最后一个结果获得的注意力（2.55）是大于倒数第二位的(1.45)，原因是用户在翻页前，对最后一条结果印象相对较深。搜索结果页面第二页排名第一的网页（即总排名11位的结果）所获得的点击只有首页排名第十网站的40%，与首页的第一条结果相比，更是只有其1/60至1/100的点击量。  

因此在量化评估搜索引擎的效果时，往往需要根据以上搜索用户的行为特点，进行针对性的设计。  
**P@N的计算方法P@N本身是Precision@N的简称，指的是对特定的查询，考虑位置因素，检测前N条结果的准确率。**例如对单次搜索的结果中前5篇，如果有4篇为相关文档，则P@5 = 4/5 = 0.8 。  
测试通常会使用一个查询集合（按照前文所述方法构造），包含若干条不同的查询词，在实际使用P@N进行评估时，通常使用所有查询的P@N数据，计算算术平均值，用来评判该系统的整体搜索结果质量。  
N的选取对用户来说，通常只关注搜索结果最前若干条结果，因此通常搜索引擎的效果评估只关注前5、或者前3结果，所以我们常用的N取值为P@3或P@5等。
对一些特定类型的查询应用，如寻址类的查询（Navigational Search），由于目标结果极为明确，因此在评估时，会选择N=1（即使用P@1）。举个例子来说，搜索“新浪网”、或“新浪首页”，如果首条结果不是 新浪网（url：www.sina.com.cn），则直接判该次查询精度不满足需求，即P@1=0。  

###MRR
上述的P@N方法，在前N结果中，排序第1位和第N位的结果，对准确率的影响是一样的。但实际情况是，**搜索引擎的评价是和排序位置极为相关的**。即排第一的结果错误，和第10位的结果错误，其严重程度有天壤之别。因此在评价系统中，需要引入位置这个因素。  
MRR是平均排序倒数（Mean Reciprocal Rank）的简称，MRR方法主要用于寻址类检索（Navigational Search）或问答类检索（Question Answering），这些检索方法只需要一个相关文档，对召回率不敏感，而是更关注搜索引擎检索到的相关文档是否排在结果列表的前面。MRR方法首先计算每一个查询的第一个相关文档位置的倒数，然后将所有倒数值求平均。（仅关注第一个相关文档，不关心检索到的相关结果的总数）。  
但对大部分检索应用来说，只有一条结果无法满足需求，对这种情况，需要更合适的方法来计算效果，其中最常用的是下述MAP方法。

###MAP
MAP方法是Mean Average Precison，即平均准确率法的简称。其定义是求每个相关文档检索出后的准确率的平均值（即Average Precision）的算术平均值（Mean）。  
**MAP 是反映系统在全部相关文档上性能的单值指标。**系统检索出来的相关文档越靠前(rank 越高)，MAP就应该越高。如果系统没有返回相关文档，则准确率默认为0。  

例如：  
假设有两个主题：主题1有4个相关网页，主题2有5个相关网页。  
某系统对于主题1检索出4个相关网页，其rank分别为1, 2, 4, 7；对于主题2检索出3个相关网页，其rank分别为1,3,5。  
对于主题1，平均准确率MAP计算公式为：(1/1+2/2+3/4+4/7)/4=0.83。   
对于主题2，平均准确率MAP计算公式为：(1/1+2/3+3/5+0+0)/5=0.45。   
则MAP= (0.83+0.45)/2=0.64。  

###DCG
DCG是英文Discounted cumulative gain的简称，中文可翻译为“折扣增益值”。DCG方法的基本思想是：

	1. 每条结果的相关性分等级来衡量
	2. 考虑结果所在的位置，位置越靠前的则重要程度越高
	3. 等级高（即好结果）的结果位置越靠前则值应该越高，否则给予惩罚

首先来看第一条：相关性分级。这里比计算Precision时简单统计“准确”或“不准确”要更为精细。我们可以将结果细分为多个等级。比如常用的3级：Good（好）、Fair（一般）、Bad（差）。对应的分值rel为：Good:3 / Fair:2 / Bad:1 。一些更为细致的评估使用5级分类法：Very Good（明显好）、Good（好）、Fair（一般）、Bad（差）、Very Bad（明显差），可以将对应分值rel设置为：Very Good:2 / Good:1 / Fair:0 / Bad:-1 / Very Bad: -2。评判结果的标准可以根据具体的应用来确定，Very Good通常是指结果的主题完全相关，并且网页内容丰富、质量很高。DCG的计算公式并不唯一，理论上只要求对数折扣因子的平滑性。例如下面这个计算公式强调了相关性，第1、2条结果的折扣系数也比较合理。  
![图片](https://lh5.googleusercontent.com/-KsIa6VbVtA8/UzepqR8sM5I/AAAAAAAAAWA/O79GJu7BR4k/w165-h58-no/%25E9%2587%258F%25E5%258C%2596%25E6%2590%259C%25E7%25B4%25A2%25E8%25B4%25A8%25E9%2587%258F3.1.jpg "DCG公式")  

取以2为底的log值也来自于经验公式，并不存在理论上的依据。实际上，Log的基数可以根据平滑的需求进行修改，当加大数值时（例如使用log5 代替log2），折扣因子降低更为迅速，此时强调了前面结果的权重。  

为了便于不同类型的query结果之间横向比较，以DCG为基础，一些评价系统还对DCG进行了归一，这些方法统称为nDCG（即 normalize DCG）。最常用的计算方法是通过除以每一个查询的理想值iDCG（ideal DCG）来进行归一。但是标定出理想的iDCG并不容易，因为每个人对“最好的结果”理解各不相同。因此实践应用中，通常选择**结果对比**这种较为简单的方法进行评估。  

##怎样实现自动化的评估？
以上所介绍的搜索引擎量化评估指标，在Cranfield评估框架（Cranfield Evaluation Framework）中被广泛使用。但Cranfield评估框架存在的问题是查询样例集合的标注上。利用手工标注答案的方式进行网络信息检索的评价是一个既耗费人力、又耗费时间的过程，只有少数大公司能够使用。并且由于搜索引擎算法改进、运营维护的需要，检索效果评价反馈的时间需要尽量缩短，因此**自动化的评测方法对提高评估效率十分重要**。最常用的自动评估方法是A/B testing系统。  

###A/B Testing
![图片](https://lh5.googleusercontent.com/-ooWZsTzLDyQ/UzelvuDAGzI/AAAAAAAAAVA/QRVo1Zdd2KE/w476-h375-no/%25E9%2587%258F%25E5%258C%2596%25E6%2590%259C%25E7%25B4%25A2%25E8%25B4%25A8%25E9%2587%258F4.jpg "A/B Testing")  
A/B testing系统在用户搜索时，由系统来自动决定用户的分组号（Bucket id），通过自动抽取流量导入不同分支，使得相应分组的用户看到的是不同产品版本（或不同搜索引擎）提供的结果。用户在不同版本产品下的行为将被记录下来，这些行为数据通过数据分析形成一系列指标，而通过这些指标的比较，最后就形成了各版本之间孰优孰劣的结论。  

在指标计算时，又可细分为两种方法，一种是基于专家评分的方法；一种是基于点击统计的方法。  
专家评分的方法通常由搜索核心技术研发和产品人员来进行，根据预先设定的标准对A、B两套环境的结果给予评分，获取每个Query的结果对比，并根据nDCG等方法计算整体质量。  
点击评分有更高的自动化程度，这里使用了一个假设：同样的排序位置，点击数量多的结果质量优于点击数量少的结果。通俗的说，相信群众。在这个假设前提下，我们可以将A/B环境前N条结果的点击率自动映射为评分，通过统计大量的Query点击结果，可以获得可靠的评分对比。  

###Interleaving Testing
另外2003年由Thorsten Joachims 等人提出的Interleaving testing方法也被广泛使用。该方法设计了一个元搜索引擎，用户输入查询词后，将查询词在几个著名搜索引擎中的查询结果随机混合反馈给用户，并收集随后用户的结果点击行为信息．根据用户不同的点击倾向性，就可以判断搜索引擎返回结果的优劣，
如下图所示，将算法A和B的结果交叉放置，并分流量进行测试，记录用户点击信息。根据点击分布来判断A和B环境的优劣。  
![图片](https://lh5.googleusercontent.com/-VGo9EOLRqeE/Uzelv9awPwI/AAAAAAAAAU8/ytP5VpRseSM/w264-h317-no/%25E9%2587%258F%25E5%258C%2596%25E6%2590%259C%25E7%25B4%25A2%25E8%25B4%25A8%25E9%2587%258F5.jpg "Interleaving Testing")  
Joachims同时证明了Interleaving Testing评价方法与传统Cranfield评价方法的结果具有较高的相关性。由于记录用户选择检索结果的行为是一个不耗费人力的过程，因此可以便捷的实现自动化的搜索效果评估。

##总结
没有评估就没有进步——对搜索效果的量化评测，目的是准确的找出现有搜索系统的不足（没有哪个搜索系统是完美的），进而一步一个脚印对算法、系统进行改进。本文为大家总结了常用的评价框架和评价指标。这些技术像一把把尺子，度量着搜索技术每一次前进的距离。
